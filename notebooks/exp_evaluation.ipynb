{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../src\n",
    "\n",
    "import os\n",
    "from common.utils import Params, set_logger, copy_weight, load_checkpoint, save_checkpoint_pos_ori, write_log, get_lr, write_train_summary_scalars, write_val_summary_joint, change_momentum\n",
    "from features.networks import TGraphNet, TGraphNetSeq\n",
    "from vizualization.vizualize import plot_adjacency_matrix, plot_pose_animation, plot_poses_only\n",
    "from common.h36m_skeleton import get_node_names, get_edge_names\n",
    "import torch\n",
    "from graph import Graph\n",
    "import numpy as np\n",
    "\n",
    "from data.h36m_dataset import Human36M\n",
    "from data.pw3d_dataset import PW3D\n",
    "from common.h36m_skeleton import joint_id_to_names, joint_names\n",
    "from data.generators import ChunkedGenerator_Seq, UnchunkedGenerator_Seq, ChunkedGenerator_Frame, ChunkedGenerator_Seq2Seq, eval_data_prepare\n",
    "from evaluation import *\n",
    "from einops import rearrange\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = \"run13\"\n",
    "description = \"baseline_root_rel\"\n",
    "exp_desc = \"eval root rel model\" \n",
    "#exp_desc= \"eval cvpr_v2/run4_1/adjacency matrices\"\n",
    "\n",
    "# Load parameters\n",
    "json_path = os.path.join('../models/stgcn/root_rel/params.json')\n",
    "assert os.path.isfile(json_path), \"No json file found at {}\".format(json_path)\n",
    "params = Params(json_path)\n",
    "\n",
    "# CUDA settings\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#params.batch_size = 1\n",
    "data_dir = \"/media/HDD3/datasets/Human3.6M/pose_zip\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TGraphNetSeq(infeat_v=params.input_node_feat,\n",
    "                  infeat_e=params.input_edge_feat,\n",
    "                  nhid_v=params.num_hidden_nodes,\n",
    "                  nhid_e=params.num_hidden_edges,\n",
    "                  n_oute=params.output_edge_feat,\n",
    "                  n_outv=params.output_node_feat,\n",
    "                  gcn_window=params.gcn_window,\n",
    "                  tcn_window=params.tcn_window,\n",
    "                  in_frames=params.in_frames,\n",
    "                  gconv_stages=params.gconv_stages,\n",
    "                  num_groups=params.num_groups,\n",
    "                  dropout=params.dropout,\n",
    "                  aggregate=params.aggregate,\n",
    "                  use_residual_connections=params.use_residual_connections,\n",
    "                  use_non_parametric=params.use_non_parametric,\n",
    "                  use_edge_conv=params.use_edge_conv,\n",
    "                  learn_adj=False)\n",
    "\n",
    "load_checkpoint('../models/stgcn/root_rel/best_pos.pth.tar', model)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['Directions', 'Discussion','Eating', 'Greeting','Phoning','Photo', 'Posing', 'Purchases','Sitting', 'SittingDown', 'Smoking', 'Waiting','WalkDog','WalkTogether','Walking', 'all']\n",
    "# actions = ['all']\n",
    "\n",
    "# Evaluating metrics\n",
    "metrics = [ mpjpe, p_mpjpe, mean_velocity_error ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_axis_mpjpe(pred, gt):\n",
    "    err_x = torch.norm(pred[:, :, :, 0] - gt[:, :, :, 0], dim = -1)\n",
    "    err_y = torch.norm(pred[:, :, :, 1] - gt[:, :, :, 1], dim = -1)\n",
    "    err_z = torch.norm(pred[:, :, :, 2] - gt[:, :, :, 2], dim = -1)\n",
    "\n",
    "    return torch.mean(err_x), torch.mean(err_y), torch.mean(err_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_axis_mpjpe(torch.zeros(32, 81, 1, 3), torch.zeros(32, 81, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log dict\n",
    "joint_errors = {}\n",
    "action_errors = {}\n",
    "action_errors_traj = {}\n",
    "\n",
    "for a in actions:\n",
    "    action_errors[a] = np.empty([0])\n",
    "    action_errors_traj[a] = np.empty([0])\n",
    "\n",
    "log_dict = {}\n",
    "log_dict['exp_name'] = exp  \n",
    "log_dict['exp_desc'] = exp_desc\n",
    "log_dict['restore_weight'] = json_path.__str__()\n",
    "\n",
    "# Loop over action\n",
    "for act in actions:\n",
    "    print(act)\n",
    "    test_dataset = Human36M(data_dir=\"/media/HDD3/datasets/Human3.6M/pose_zip\", train=False, ds_category=params.ds_category, actions=act)\n",
    "\n",
    "    cam, pos2d, pos3d, angles_6d, edge_features = test_dataset.cam, test_dataset.pos2d, test_dataset.pos3d_centered, [], []\n",
    "    val_generator = ChunkedGenerator_Seq2Seq(params.batch_size, cameras=None, poses_2d=pos2d, poses_3d=pos3d,\n",
    "                                                   chunk_length=31, pad=25, out_all=True, shuffle=False,\n",
    "                                                   augment=False, reverse_aug=False,)\n",
    "\n",
    "    # joint id to name mapping\n",
    "    joint_id_to_names = test_dataset.joint_id_to_names\n",
    "    total_errors = np.empty([0])\n",
    "    total_errors_traj = np.empty([0])\n",
    "    for j in joint_names:\n",
    "        joint_errors[j] = np.empty([0])\n",
    "\n",
    "    # model forward\n",
    "    err_val_p2_list = []\n",
    "    summary = []\n",
    "    joint_dict = joint_id_to_names\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    total_mpjpe = 0\n",
    "    total_pmpjpe = 0\n",
    "    total_err_traj = 0\n",
    "    total_velocity = 0\n",
    "    total_err_x = 0\n",
    "    total_err_y = 0\n",
    "    total_err_z = 0\n",
    "    N = 0\n",
    "    n_batches = 0\n",
    "    per_joint_err = torch.zeros(17)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for cameras_val, batch_3d, batch_6d, batch_2d, batch_edge in val_generator.next_epoch():\n",
    "            input_2d = torch.FloatTensor(batch_2d).to(device)\n",
    "            target_pose_3d = torch.FloatTensor(batch_3d).to(device)\n",
    "            # cameras_val = torch.from_numpy(cameras_val.astype('float32')).to(device)\n",
    "\n",
    "            middle_index = int((target_pose_3d.shape[1] - 1) / 2)\n",
    "            pad = 15\n",
    "            start_index = middle_index - pad\n",
    "            end_index = middle_index + pad + 1\n",
    "            B, T, J, D = target_pose_3d.shape\n",
    "\n",
    "            predicted_pos3d = model(input_2d)\n",
    "            predicted_pos3d_center = predicted_pos3d[:, start_index:end_index].reshape(B, (2 * pad + 1), J, D)\n",
    "\n",
    "            # target_angle_6d = target_angle_6d[:, middle_index].view_as(predicted_angle_6d)\n",
    "            target_pose_3d = target_pose_3d.view_as(predicted_pos3d)\n",
    "            target_pose_3d_center = target_pose_3d[:, start_index:end_index].reshape(B, (2 * pad + 1), J, D)\n",
    "\n",
    "            target_pose_3d_center[:, :, :1] = 0\n",
    "            predicted_pos3d_center[:, :, :1] = 0\n",
    "\n",
    "            errors = torch.norm(target_pose_3d_center - predicted_pos3d_center, dim=len(predicted_pos3d_center.shape)-1)\n",
    "            err_x, err_y, err_z = per_axis_mpjpe(target_pose_3d_center[:, :, :1], predicted_pos3d_center[:, :, :1])\n",
    "\n",
    "            total_err_x += err_x * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            total_err_y += err_y * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            total_err_z += err_z * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            # errors: [B, T, N]\n",
    "            total_errors = np.concatenate((total_errors, errors.mean(dim=-1).reshape(-1).cpu().data.numpy()))\n",
    "            total_errors_traj = np.concatenate((total_errors_traj, errors[:, :, :1].mean(dim=-1).reshape(-1).cpu().data.numpy()))\n",
    "            action_errors[act] = np.concatenate((action_errors[act], errors.mean(dim=-1).reshape(-1).cpu().data.numpy()))\n",
    "            action_errors_traj[act] = np.concatenate((action_errors_traj[act], errors[:, :, :1].mean(dim=-1).reshape(-1).cpu().data.numpy()))\n",
    "            errors = rearrange(errors, 'B T N -> N (B T)').cpu().data.numpy()\n",
    "            for joint_id in range(errors.shape[0]):\n",
    "                joint_errors[joint_dict[joint_id]] = np.concatenate((joint_errors[joint_dict[joint_id]], errors[joint_id]), axis=0)\n",
    "\n",
    "            n_batches += 1\n",
    "\n",
    "            err_pos, joint_err = metrics[0](\n",
    "                predicted_pos3d_center.cpu().data,\n",
    "                target_pose_3d_center.cpu().data\n",
    "            )\n",
    "\n",
    "            err_vel = metrics[2](\n",
    "                predicted_pos3d_center.cpu().data,\n",
    "                target_pose_3d_center.cpu().data,\n",
    "                1\n",
    "            )\n",
    "\n",
    "            total_mpjpe += err_pos.cpu().data.numpy() * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            per_joint_err += joint_err * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            N += predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "\n",
    "            total_velocity += err_vel.cpu().data.numpy() * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "            # p-mpjpe\n",
    "            err_pos_p = metrics[1](\n",
    "                predicted_pos3d_center.cpu().data.numpy().reshape(-1, 17, 3),\n",
    "                target_pose_3d_center.cpu().data.numpy().reshape(-1, 17, 3)\n",
    "            )\n",
    "\n",
    "            total_pmpjpe += err_pos_p * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "\n",
    "            err_traj, _ = metrics[0](\n",
    "                target_pose_3d_center[:, :, :1].cpu().data,\n",
    "                predicted_pos3d_center[:, :, :1].cpu().data\n",
    "            )\n",
    "\n",
    "            total_err_traj += err_traj.cpu().data.numpy() * predicted_pos3d.shape[0] * predicted_pos3d.shape[1]\n",
    "\n",
    "\n",
    "    # Average scores \n",
    "\n",
    "    # joint level err\n",
    "    # log entry\n",
    "    log_dict[act] = {}\n",
    "    log_dict[act]['p1_err'] = total_mpjpe / N\n",
    "    log_dict[act]['p2_err'] = total_pmpjpe / N\n",
    "    log_dict[act]['traj_err'] = total_err_traj / N\n",
    "    log_dict[act]['velocity_err'] = total_velocity / N\n",
    "    log_dict[act]['err_x'] = total_err_x / N\n",
    "    log_dict[act]['err_y'] = total_err_y / N\n",
    "    log_dict[act]['err_z'] = total_err_z / N\n",
    "\n",
    "    mean_err_joint = per_joint_err / N\n",
    "\n",
    "    log_dict[act]['p1_err_joint']={}\n",
    "    for joint_id in range(len(mean_err_joint)):\n",
    "        log_dict[act]['p1_err_joint'][joint_dict[joint_id]] = {}\n",
    "        log_dict[act]['p1_err_joint'][joint_dict[joint_id]]['mean'] = mean_err_joint[joint_id].cpu().data.numpy().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_errors_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# write log to file\n",
    "log_dict_fname = os.path.join(\"../reports/evaluation_results/\", f\"eval_action_{exp}_{description}_root_included.json\")\n",
    "with open(log_dict_fname, 'w') as fp:\n",
    "        json.dump(log_dict, fp, sort_keys=True, indent=4) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# read log file from disk\n",
    "log_dict_fname = os.path.join(\"../reports/evaluation_results/\", f\"eval_action_run14_trajectory_model.json\")\n",
    "with open(log_dict_fname, 'r') as fp:\n",
    "    log_dict = json.load(fp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Joint Wise Bar Plot\n",
    "## MPJPE Error - Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defaultPlotting(): sns.set(rc={'figure.figsize':(11.7,8.27),\"font.size\":20,\"axes.titlesize\":20,\"axes.labelsize\":20},style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_dup = total_errors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors_dup[errors_dup > 250] = 250\n",
    "errors_dup[errors_dup < 25] = 20\n",
    "errors_dup[errors_dup > 75] = 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "defaultPlotting()\n",
    "\n",
    "ax = sns.histplot(x=errors_dup, fill=\"blue\", element=\"step\", stat=\"percent\", binwidth=5, binrange=[20, 75])\n",
    "# ax = sns.violinplot(x=errors_dup,)\n",
    "ax.set_xticks([20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75])\n",
    "# ax.set_xticklabels([0, 0, 50, 100, 150, 200, \"> 250\"], fontsize=20)\n",
    "ax.set_xlabel(\"The Range of MPJPE (mm)\", fontsize=24, fontdict={'weight': 'bold'})\n",
    "ax.set_title(\"Distribution of MPJPE\", fontsize=24, fontdict={'weight': 'bold'})\n",
    "ax.set_ylabel(\"Percent\", fontsize=24, fontdict={'weight': 'bold'})\n",
    "ax.set_yticklabels(ax.get_yticks(), fontsize=20)\n",
    "ax.set_xticklabels([\"< 25\", 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, \"> 75\"], fontsize=20)\n",
    "\n",
    "plt.savefig(\"../reports/figures/mpjpe_dist.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight': 'bold'}\n",
    "defaultPlotting()\n",
    "\n",
    "for a in actions:    \n",
    "    errors_dup = action_errors[a].copy()\n",
    "    # errors_dup[errors_dup > 250] = 250\n",
    "    errors_dup[errors_dup < 25] = 20\n",
    "    errors_dup[errors_dup > 75] = 71\n",
    "\n",
    "    ax = sns.histplot(x=errors_dup, fill=\"blue\", bins=[20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75], element=\"poly\", stat=\"percent\",)\n",
    "    # ax = sns.violinplot(x=errors_dup)\n",
    "    # ax.set_xlim([0, 250])\n",
    "    ax.set_xticks([20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75])\n",
    "    ax.set_xticklabels([\"< 25\", 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, \"> 75\"], fontsize=20)\n",
    "    ax.set_xlabel(\"MPJPE (mm)\", fontsize=24, fontdict=font)\n",
    "    ax.set_title(f\"Distribution of MPJPE ({a})\", fontsize=24, fontdict=font)\n",
    "    ax.set_ylabel(\"Percent\", fontsize=24, fontdict=font)\n",
    "    ax.set_yticklabels(ax.get_yticks(), fontsize=20)\n",
    "\n",
    "    plt.savefig(f\"../reports/figures/mpjpe_dist_{a}_hplot.png\", dpi=300, bbox_inches='tight', pad_inches = 0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = ['WalkTogether']\n",
    "actions = ['Directions', 'Discussion','Eating', 'Greeting','Phoning','Photo', 'Posing', 'Purchases',\\\n",
    "   'Sitting', 'SittingDown', 'Smoking', 'Waiting','WalkDog','Walking','WalkTogether', 'all']\n",
    "\n",
    "# print on screen\n",
    "print(\"action\", \"p1\", \"p2\")\n",
    "for act in actions:\n",
    "    act_scores = log_dict[act]\n",
    "    #print(act_scores)\n",
    "    print(act, \",\", np.round(act_scores['p1_err'], 2), \",\", np.round(act_scores['p2_err'], 2))\n",
    "\n",
    "# for latex\n",
    "header = \"\\\\textbf{Protocol \\#1} \\t\"\n",
    "score_rec_p1 = \"Ours (P1) \\t\"\n",
    "score_rec_p2 = \"Ours (P2) \\t\"\n",
    "score_rec_vel = \"Ours (Vel) \\t\"\n",
    "\n",
    "for act in actions:\n",
    "    act_scores = log_dict[act]\n",
    "    header+=\" & \"+act\n",
    "    score_rec_p1+=\" & \"+str(np.round(act_scores['p1_err'], 1))\n",
    "    score_rec_p2+=\" & \"+str(np.round(act_scores['p2_err'], 1))\n",
    "    score_rec_vel+=\" & \"+str(np.round(act_scores['velocity_err'], 1))\n",
    "\n",
    "\n",
    "print(header)\n",
    "print(score_rec_p1)\n",
    "print(score_rec_p2)\n",
    "print(score_rec_vel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions = ['WalkTogether']\n",
    "actions = ['Directions', 'Discussion','Eating', 'Greeting','Phoning','Photo', 'Posing', 'Purchases',\\\n",
    "   'Sitting', 'SittingDown', 'Smoking', 'Waiting','WalkDog','Walking','WalkTogether']\n",
    "\n",
    "# print on screen\n",
    "print(\"action\", \"p1\", \"p2\")\n",
    "for act in actions:\n",
    "    act_scores = log_dict[act]\n",
    "    #print(act_scores)\n",
    "    print(act, \",\", np.round(act_scores['p1_err'], 2), \",\", np.round(act_scores['p2_err'], 2))\n",
    "\n",
    "# for latex\n",
    "header = \"\\\\textbf{Protocol \\#1} \\t\"\n",
    "score_rec_p1 = \"Ours (P1) \\t\"\n",
    "score_rec_p2 = \"Ours (P2) \\t\"\n",
    "score_rec_vel = \"Ours (Vel) \\t\"\n",
    "\n",
    "for act in actions:\n",
    "    act_scores = log_dict[act]\n",
    "    header+=\" & \"+act\n",
    "    score_rec_p1+=\" & \"+str(np.round(act_scores['p1_err'], 1))\n",
    "    score_rec_p2+=\" & \"+str(np.round(act_scores['p2_err'], 1))\n",
    "    score_rec_vel+=\" & \"+str(np.round(act_scores['velocity_err'], 1))\n",
    "\n",
    "\n",
    "print(header)\n",
    "print(score_rec_p1)\n",
    "print(score_rec_p2)\n",
    "print(score_rec_vel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for jname, jid in zip(joint_names, joint_id_to_names.keys()):\n",
    "    errs = joint_errors[jname]\n",
    "    mu, std = np.mean(errs), np.std(errs)\n",
    "    res_dict[jname] = [mu, std / 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "\n",
    "for act in actions:\n",
    "    if act == \"\"\n",
    "    errs = action_errors_traj[act]\n",
    "    mu, std = np.mean(errs), np.std(errs)\n",
    "    res_dict[act] = [mu, std / 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale = 1.4)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 9))\n",
    "# defaultPlotting()\n",
    "ax = sns.barplot(x=list(res_dict.keys())[1:], y=list(map(lambda v: v[0], res_dict.values()))[1:], color='steelblue')\n",
    "\n",
    "font = {'weight': 'bold'}\n",
    "ax.axes.set_title(\"Per Action Errors\",fontsize=20, fontdict={'weight': 'bold'})\n",
    "ax.set_xlabel(\"Action\",fontsize=20, fontdict=font)\n",
    "ax.set_ylabel(\"MPJPE\",fontsize=20, fontdict=font)\n",
    "ax.tick_params(labelsize=20)\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.savefig(\"../reports/figures/per_action_err_traj.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(font_scale = 1.4)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 9))\n",
    "# defaultPlotting()\n",
    "ax = sns.barplot(x=list(res_dict.keys())[1:], y=list(map(lambda v: v[0], res_dict.values()))[1:], color='steelblue')\n",
    "\n",
    "font = {'weight': 'bold'}\n",
    "ax.axes.set_title(\"Per Joint Errors\",fontsize=24, fontdict={'weight': 'bold'})\n",
    "ax.set_xlabel(\"Action\",fontsize=24, fontdict=font)\n",
    "ax.set_ylabel(\"MPJPE\",fontsize=24, fontdict=font)\n",
    "ax.tick_params(labelsize=22)\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "\n",
    "plt.savefig(\"../reports/figures/per_joint_err.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_input1 = torch.randn(1, 81, 17, 2, dtype=torch.float).to(device)\n",
    "\n",
    "# INIT LOGGERS\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 1000\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP\n",
    "for _ in range(10):\n",
    "    _ = model(dummy_input1)\n",
    "\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input1)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(mean_syn, \" ms\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops = FlopCountAnalysis(model.cpu(), torch.randn(1, 81, 17, 2, dtype=torch.float).cpu())\n",
    "flops.total() / 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops.by_operator(), flops.by_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
